{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os, time\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xT0dHZ3S4SWR",
        "outputId": "a1293ee5-b0a7-4d39-e0d0-5ced0ea311c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19H5aSgN2iTi",
        "outputId": "9da8f347-c8ab-4711-8e11-fef04a221ba2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reef_dir contents: ['reef 64', 'reef 70']\n",
            "sand_dir contents: ['sand 64', 'sand 70']\n",
            "Found 861 reef images and 1504 sand images\n",
            "Train: 1655 Val: 355 Test: 355\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-8ef52f8b2e53>:214: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scalers[name]    = GradScaler()\n",
            "<ipython-input-14-8ef52f8b2e53>:231: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Baseline_ReLU6: TrL=0.0769 TrA=0.8876  ValL=0.0108 ValA=0.9915\n",
            "  LeakyReLU: TrL=0.0674 TrA=0.8900  ValL=0.0067 ValA=0.9859\n",
            "  GeLU: TrL=0.1115 TrA=0.8254  ValL=0.0403 ValA=0.9606\n",
            "  SiLU: TrL=0.1258 TrA=0.7861  ValL=0.0564 ValA=0.9380\n",
            "  Extra_FFN: TrL=0.0658 TrA=0.8979  ValL=0.0121 ValA=0.9915\n",
            "  High_Dropout: TrL=0.1121 TrA=0.8363  ValL=0.0072 ValA=0.9915\n",
            "------------------------------------------------------------\n",
            "Epoch 2/50\n",
            "  Baseline_ReLU6: TrL=0.0320 TrA=0.9601  ValL=0.0116 ValA=0.9859\n",
            "  LeakyReLU: TrL=0.0335 TrA=0.9553  ValL=0.0137 ValA=0.9915\n",
            "  GeLU: TrL=0.0846 TrA=0.8598  ValL=0.0213 ValA=0.9859\n",
            "  SiLU: TrL=0.0945 TrA=0.8508  ValL=0.0366 ValA=0.9549\n",
            "  Extra_FFN: TrL=0.0314 TrA=0.9589  ValL=0.0118 ValA=0.9915\n",
            "  High_Dropout: TrL=0.0462 TrA=0.9384  ValL=0.0077 ValA=0.9915\n",
            "------------------------------------------------------------\n",
            "Epoch 3/50\n",
            "  Baseline_ReLU6: TrL=0.0285 TrA=0.9583  ValL=0.0169 ValA=0.9775\n",
            "  LeakyReLU: TrL=0.0323 TrA=0.9571  ValL=0.0119 ValA=0.9944\n",
            "  GeLU: TrL=0.0622 TrA=0.9088  ValL=0.0219 ValA=0.9746\n",
            "  SiLU: TrL=0.0793 TrA=0.8695  ValL=0.0465 ValA=0.9577\n",
            "  Extra_FFN: TrL=0.0239 TrA=0.9704  ValL=0.0183 ValA=0.9859\n",
            "  High_Dropout: TrL=0.0367 TrA=0.9517  ValL=0.0095 ValA=0.9915\n",
            "------------------------------------------------------------\n",
            "Epoch 4/50\n",
            "  Baseline_ReLU6: TrL=0.0362 TrA=0.9535  ValL=0.0052 ValA=0.9944\n",
            "  LeakyReLU: TrL=0.0318 TrA=0.9619  ValL=0.0094 ValA=0.9944\n",
            "  GeLU: TrL=0.0604 TrA=0.9088  ValL=0.0137 ValA=0.9775\n",
            "  SiLU: TrL=0.0857 TrA=0.8616  ValL=0.0214 ValA=0.9746\n",
            "  Extra_FFN: TrL=0.0244 TrA=0.9625  ValL=0.0111 ValA=0.9887\n",
            "  High_Dropout: TrL=0.0346 TrA=0.9565  ValL=0.0091 ValA=0.9915\n",
            "------------------------------------------------------------\n",
            "Epoch 5/50\n",
            "  Baseline_ReLU6: TrL=0.0231 TrA=0.9716  ValL=0.0080 ValA=0.9944\n",
            "  LeakyReLU: TrL=0.0255 TrA=0.9644  ValL=0.0100 ValA=0.9944\n",
            "  GeLU: TrL=0.0542 TrA=0.9257  ValL=0.0422 ValA=0.9437\n",
            "  SiLU: TrL=0.0671 TrA=0.8894  ValL=0.0165 ValA=0.9831\n",
            "  Extra_FFN: TrL=0.0195 TrA=0.9710  ValL=0.0146 ValA=0.9887\n",
            "  High_Dropout: TrL=0.0397 TrA=0.9486  ValL=0.0119 ValA=0.9915\n",
            "------------------------------------------------------------\n",
            "Epoch 6/50\n",
            "  Baseline_ReLU6: TrL=0.0244 TrA=0.9674  ValL=0.0121 ValA=0.9887\n",
            "  LeakyReLU: TrL=0.0259 TrA=0.9637  ValL=0.0142 ValA=0.9944\n",
            "  GeLU: TrL=0.0530 TrA=0.9341  ValL=0.0161 ValA=0.9746\n",
            "  SiLU: TrL=0.0607 TrA=0.9112  ValL=0.0161 ValA=0.9887\n",
            "  Extra_FFN: TrL=0.0204 TrA=0.9674  ValL=0.0105 ValA=0.9915\n",
            "  High_Dropout: TrL=0.0326 TrA=0.9583  ValL=0.0108 ValA=0.9944\n",
            "------------------------------------------------------------\n",
            "Epoch 7/50\n",
            "  Baseline_ReLU6: TrL=0.0251 TrA=0.9680  ValL=0.0109 ValA=0.9944\n",
            "  LeakyReLU: TrL=0.0232 TrA=0.9686  ValL=0.0078 ValA=0.9944\n",
            "  GeLU: TrL=0.0475 TrA=0.9311  ValL=0.0305 ValA=0.9746\n",
            "  SiLU: TrL=0.0717 TrA=0.8894  ValL=0.0234 ValA=0.9831\n",
            "  Extra_FFN: TrL=0.0262 TrA=0.9613  ValL=0.0098 ValA=0.9915\n",
            "  High_Dropout: TrL=0.0266 TrA=0.9619  ValL=0.0166 ValA=0.9915\n",
            "------------------------------------------------------------\n",
            "Epoch 8/50\n",
            "  Baseline_ReLU6: TrL=0.0230 TrA=0.9710  ValL=0.0065 ValA=0.9944\n",
            "  LeakyReLU: TrL=0.0235 TrA=0.9650  ValL=0.0074 ValA=0.9887\n",
            "  GeLU: TrL=0.0529 TrA=0.9317  ValL=0.0166 ValA=0.9746\n",
            "  SiLU: TrL=0.0657 TrA=0.9045  ValL=0.0216 ValA=0.9690\n",
            "  Extra_FFN: TrL=0.0220 TrA=0.9734  ValL=0.0115 ValA=0.9887\n",
            "  High_Dropout: TrL=0.0252 TrA=0.9625  ValL=0.0102 ValA=0.9915\n",
            "------------------------------------------------------------\n",
            "Epoch 9/50\n",
            "  Baseline_ReLU6: TrL=0.0230 TrA=0.9716  ValL=0.0061 ValA=0.9944\n",
            "  LeakyReLU: TrL=0.0202 TrA=0.9746  ValL=0.0117 ValA=0.9887\n",
            "  GeLU: TrL=0.0418 TrA=0.9414  ValL=0.0387 ValA=0.9662\n",
            "  SiLU: TrL=0.0572 TrA=0.9215  ValL=0.0161 ValA=0.9690\n",
            "  Extra_FFN: TrL=0.0231 TrA=0.9692  ValL=0.0140 ValA=0.9944\n",
            "  High_Dropout: TrL=0.0260 TrA=0.9686  ValL=0.0081 ValA=0.9859\n",
            "------------------------------------------------------------\n",
            "Epoch 10/50\n",
            "  Baseline_ReLU6: TrL=0.0246 TrA=0.9668  ValL=0.0113 ValA=0.9944\n",
            "  LeakyReLU: TrL=0.0222 TrA=0.9710  ValL=0.0076 ValA=0.9944\n",
            "  GeLU: TrL=0.0443 TrA=0.9353  ValL=0.0166 ValA=0.9831\n",
            "  SiLU: TrL=0.0545 TrA=0.9202  ValL=0.0161 ValA=0.9887\n",
            "  Extra_FFN: TrL=0.0166 TrA=0.9740  ValL=0.0164 ValA=0.9887\n",
            "  High_Dropout: TrL=0.0336 TrA=0.9625  ValL=0.0101 ValA=0.9944\n",
            "------------------------------------------------------------\n",
            "Epoch 11/50\n",
            "  Baseline_ReLU6: TrL=0.0304 TrA=0.9656  ValL=0.0100 ValA=0.9859\n",
            "  LeakyReLU: TrL=0.0233 TrA=0.9692  ValL=0.0100 ValA=0.9887\n",
            "  GeLU: TrL=0.0457 TrA=0.9341  ValL=0.0140 ValA=0.9859\n",
            "  SiLU: TrL=0.0515 TrA=0.9335  ValL=0.0383 ValA=0.9718\n",
            "  Extra_FFN: TrL=0.0208 TrA=0.9704  ValL=0.0118 ValA=0.9915\n",
            "  High_Dropout: TrL=0.0197 TrA=0.9716  ValL=0.0112 ValA=0.9915\n",
            "------------------------------------------------------------\n",
            "Epoch 12/50\n",
            "  Baseline_ReLU6: TrL=0.0176 TrA=0.9776  ValL=0.0064 ValA=0.9915\n",
            "  LeakyReLU: TrL=0.0265 TrA=0.9704  ValL=0.0120 ValA=0.9831\n",
            "  GeLU: TrL=0.0371 TrA=0.9438  ValL=0.0223 ValA=0.9803\n",
            "  SiLU: TrL=0.0526 TrA=0.9299  ValL=0.0411 ValA=0.9577\n",
            "  Extra_FFN: TrL=0.0194 TrA=0.9734  ValL=0.0100 ValA=0.9944\n",
            "  High_Dropout: TrL=0.0186 TrA=0.9728  ValL=0.0101 ValA=0.9915\n",
            "------------------------------------------------------------\n",
            "Epoch 13/50\n",
            "  Baseline_ReLU6: TrL=0.0194 TrA=0.9752  ValL=0.0080 ValA=0.9915\n",
            "  LeakyReLU: TrL=0.0173 TrA=0.9789  ValL=0.0151 ValA=0.9859\n",
            "  GeLU: TrL=0.0356 TrA=0.9529  ValL=0.0197 ValA=0.9831\n",
            "  SiLU: TrL=0.0506 TrA=0.9233  ValL=0.0262 ValA=0.9606\n",
            "  Extra_FFN: TrL=0.0148 TrA=0.9807  ValL=0.0113 ValA=0.9831\n",
            "  High_Dropout: TrL=0.0153 TrA=0.9789  ValL=0.0128 ValA=0.9831\n",
            "------------------------------------------------------------\n",
            "Epoch 14/50\n",
            "  Baseline_ReLU6: TrL=0.0216 TrA=0.9716  ValL=0.0100 ValA=0.9915\n",
            "  LeakyReLU: TrL=0.0176 TrA=0.9825  ValL=0.0084 ValA=0.9887\n",
            "  GeLU: TrL=0.0364 TrA=0.9486  ValL=0.0207 ValA=0.9831\n",
            "  SiLU: TrL=0.0529 TrA=0.9251  ValL=0.0195 ValA=0.9718\n",
            "  Extra_FFN: TrL=0.0165 TrA=0.9849  ValL=0.0092 ValA=0.9915\n",
            "  High_Dropout: TrL=0.0190 TrA=0.9764  ValL=0.0105 ValA=0.9887\n",
            "------------------------------------------------------------\n",
            "Epoch 15/50\n",
            "  Baseline_ReLU6: TrL=0.0155 TrA=0.9770  ValL=0.0080 ValA=0.9944\n",
            "  LeakyReLU: TrL=0.0230 TrA=0.9680  ValL=0.0092 ValA=0.9915\n",
            "  GeLU: TrL=0.0359 TrA=0.9517  ValL=0.0173 ValA=0.9859\n",
            "  SiLU: TrL=0.0455 TrA=0.9269  ValL=0.0215 ValA=0.9859\n",
            "  Extra_FFN: TrL=0.0208 TrA=0.9740  ValL=0.0086 ValA=0.9915\n",
            "  High_Dropout: TrL=0.0226 TrA=0.9674  ValL=0.0125 ValA=0.9859\n",
            "------------------------------------------------------------\n",
            "Epoch 16/50\n",
            "  Baseline_ReLU6: TrL=0.0151 TrA=0.9795  ValL=0.0072 ValA=0.9944\n",
            "  LeakyReLU: TrL=0.0182 TrA=0.9746  ValL=0.0076 ValA=0.9887\n",
            "  GeLU: TrL=0.0309 TrA=0.9529  ValL=0.0178 ValA=0.9831\n",
            "  SiLU: TrL=0.0425 TrA=0.9438  ValL=0.0261 ValA=0.9718\n",
            "  Extra_FFN: TrL=0.0153 TrA=0.9776  ValL=0.0081 ValA=0.9915\n",
            "  High_Dropout: TrL=0.0204 TrA=0.9698  ValL=0.0112 ValA=0.9887\n",
            "------------------------------------------------------------\n",
            "Epoch 17/50\n",
            "  Baseline_ReLU6: TrL=0.0163 TrA=0.9849  ValL=0.0069 ValA=0.9915\n",
            "  LeakyReLU: TrL=0.0133 TrA=0.9843  ValL=0.0074 ValA=0.9887\n",
            "  GeLU: TrL=0.0341 TrA=0.9432  ValL=0.0158 ValA=0.9887\n",
            "  SiLU: TrL=0.0450 TrA=0.9305  ValL=0.0183 ValA=0.9803\n",
            "  Extra_FFN: TrL=0.0182 TrA=0.9782  ValL=0.0202 ValA=0.9831\n",
            "  High_Dropout: TrL=0.0185 TrA=0.9758  ValL=0.0073 ValA=0.9915\n",
            "------------------------------------------------------------\n",
            "Epoch 18/50\n",
            "  Baseline_ReLU6: TrL=0.0180 TrA=0.9710  ValL=0.0055 ValA=0.9915\n",
            "  LeakyReLU: TrL=0.0214 TrA=0.9728  ValL=0.0072 ValA=0.9887\n",
            "  GeLU: TrL=0.0312 TrA=0.9577  ValL=0.0188 ValA=0.9859\n",
            "  SiLU: TrL=0.0412 TrA=0.9378  ValL=0.0213 ValA=0.9775\n",
            "  Extra_FFN: TrL=0.0153 TrA=0.9801  ValL=0.0084 ValA=0.9944\n",
            "  High_Dropout: TrL=0.0143 TrA=0.9819  ValL=0.0094 ValA=0.9915\n",
            "------------------------------------------------------------\n",
            "Epoch 19/50\n",
            "  Baseline_ReLU6: TrL=0.0160 TrA=0.9770  ValL=0.0069 ValA=0.9944\n",
            "  LeakyReLU: TrL=0.0179 TrA=0.9752  ValL=0.0055 ValA=0.9915\n",
            "  GeLU: TrL=0.0338 TrA=0.9601  ValL=0.0170 ValA=0.9859\n",
            "  SiLU: TrL=0.0394 TrA=0.9492  ValL=0.0269 ValA=0.9690\n",
            "  Extra_FFN: TrL=0.0140 TrA=0.9782  ValL=0.0064 ValA=0.9944\n",
            "  High_Dropout: TrL=0.0169 TrA=0.9740  ValL=0.0123 ValA=0.9915\n",
            "------------------------------------------------------------\n",
            "Epoch 20/50\n",
            "  Baseline_ReLU6: TrL=0.0185 TrA=0.9770  ValL=0.0127 ValA=0.9915\n",
            "  LeakyReLU: TrL=0.0150 TrA=0.9825  ValL=0.0053 ValA=0.9915\n",
            "  GeLU: TrL=0.0295 TrA=0.9583  ValL=0.0167 ValA=0.9859\n",
            "  SiLU: TrL=0.0487 TrA=0.9305  ValL=0.0202 ValA=0.9803\n",
            "  Extra_FFN: TrL=0.0114 TrA=0.9855  ValL=0.0070 ValA=0.9915\n",
            "  High_Dropout: TrL=0.0221 TrA=0.9764  ValL=0.0046 ValA=0.9915\n",
            "------------------------------------------------------------\n",
            "Epoch 21/50\n",
            "  Baseline_ReLU6: TrL=0.0170 TrA=0.9740  ValL=0.0042 ValA=0.9915\n",
            "  LeakyReLU: TrL=0.0190 TrA=0.9782  ValL=0.0073 ValA=0.9859\n",
            "  GeLU: TrL=0.0310 TrA=0.9637  ValL=0.0196 ValA=0.9831\n",
            "  SiLU: TrL=0.0420 TrA=0.9353  ValL=0.0175 ValA=0.9746\n",
            "  Extra_FFN: TrL=0.0102 TrA=0.9879  ValL=0.0090 ValA=0.9915\n",
            "  High_Dropout: TrL=0.0215 TrA=0.9716  ValL=0.0076 ValA=0.9944\n",
            "------------------------------------------------------------\n",
            "Epoch 22/50\n",
            "  Baseline_ReLU6: TrL=0.0182 TrA=0.9764  ValL=0.0068 ValA=0.9915\n",
            "  LeakyReLU: TrL=0.0144 TrA=0.9782  ValL=0.0054 ValA=0.9887\n",
            "  GeLU: TrL=0.0280 TrA=0.9613  ValL=0.0190 ValA=0.9887\n",
            "  SiLU: TrL=0.0435 TrA=0.9341  ValL=0.0224 ValA=0.9831\n",
            "  Extra_FFN: TrL=0.0144 TrA=0.9831  ValL=0.0074 ValA=0.9915\n",
            "  High_Dropout: TrL=0.0165 TrA=0.9770  ValL=0.0060 ValA=0.9944\n",
            "------------------------------------------------------------\n",
            "Epoch 23/50\n",
            "  Baseline_ReLU6: TrL=0.0124 TrA=0.9849  ValL=0.0064 ValA=0.9915\n",
            "  LeakyReLU: TrL=0.0163 TrA=0.9789  ValL=0.0049 ValA=0.9915\n",
            "  GeLU: TrL=0.0253 TrA=0.9674  ValL=0.0191 ValA=0.9887\n",
            "  SiLU: TrL=0.0291 TrA=0.9631  ValL=0.0333 ValA=0.9718\n",
            "  Extra_FFN: TrL=0.0101 TrA=0.9849  ValL=0.0070 ValA=0.9915\n",
            "  High_Dropout: TrL=0.0197 TrA=0.9776  ValL=0.0098 ValA=0.9944\n",
            "------------------------------------------------------------\n",
            "Epoch 24/50\n",
            "  Baseline_ReLU6: TrL=0.0141 TrA=0.9813  ValL=0.0057 ValA=0.9915\n",
            "  LeakyReLU: TrL=0.0237 TrA=0.9776  ValL=0.0150 ValA=0.9746\n",
            "  GeLU: TrL=0.0414 TrA=0.9414  ValL=0.0244 ValA=0.9831\n",
            "  SiLU: TrL=0.0505 TrA=0.9251  ValL=0.0149 ValA=0.9887\n",
            "  Extra_FFN: TrL=0.0145 TrA=0.9831  ValL=0.0089 ValA=0.9915\n",
            "  High_Dropout: TrL=0.0212 TrA=0.9752  ValL=0.0057 ValA=0.9915\n",
            "------------------------------------------------------------\n",
            "Epoch 25/50\n",
            "  Baseline_ReLU6: TrL=0.0134 TrA=0.9831  ValL=0.0067 ValA=0.9944\n",
            "  LeakyReLU: TrL=0.0169 TrA=0.9801  ValL=0.0079 ValA=0.9887\n",
            "  GeLU: TrL=0.0314 TrA=0.9541  ValL=0.0164 ValA=0.9887\n",
            "  SiLU: TrL=0.0345 TrA=0.9426  ValL=0.0215 ValA=0.9831\n",
            "  Extra_FFN: TrL=0.0156 TrA=0.9813  ValL=0.0111 ValA=0.9915\n",
            "  High_Dropout: TrL=0.0215 TrA=0.9734  ValL=0.0092 ValA=0.9915\n",
            "------------------------------------------------------------\n",
            "Epoch 26/50\n",
            "  Baseline_ReLU6: TrL=0.0133 TrA=0.9813  ValL=0.0076 ValA=0.9915\n",
            "  LeakyReLU: TrL=0.0155 TrA=0.9813  ValL=0.0069 ValA=0.9915\n",
            "  GeLU: TrL=0.0356 TrA=0.9517  ValL=0.0178 ValA=0.9831\n",
            "  SiLU: TrL=0.0351 TrA=0.9523  ValL=0.0183 ValA=0.9859\n",
            "  Extra_FFN: TrL=0.0144 TrA=0.9843  ValL=0.0049 ValA=0.9915\n",
            "  High_Dropout: TrL=0.0147 TrA=0.9801  ValL=0.0099 ValA=0.9915\n",
            "------------------------------------------------------------\n",
            "Epoch 27/50\n",
            "  Baseline_ReLU6: TrL=0.0142 TrA=0.9801  ValL=0.0066 ValA=0.9944\n",
            "  LeakyReLU: TrL=0.0130 TrA=0.9831  ValL=0.0056 ValA=0.9887\n",
            "  GeLU: TrL=0.0266 TrA=0.9668  ValL=0.0156 ValA=0.9859\n",
            "  SiLU: TrL=0.0390 TrA=0.9498  ValL=0.0231 ValA=0.9831\n",
            "  Extra_FFN: TrL=0.0125 TrA=0.9825  ValL=0.0078 ValA=0.9915\n",
            "  High_Dropout: TrL=0.0166 TrA=0.9782  ValL=0.0049 ValA=0.9944\n",
            "------------------------------------------------------------\n",
            "Epoch 28/50\n",
            "  Baseline_ReLU6: TrL=0.0136 TrA=0.9825  ValL=0.0076 ValA=0.9915\n",
            "  LeakyReLU: TrL=0.0100 TrA=0.9825  ValL=0.0067 ValA=0.9915\n",
            "  GeLU: TrL=0.0282 TrA=0.9644  ValL=0.0138 ValA=0.9859\n",
            "  SiLU: TrL=0.0368 TrA=0.9462  ValL=0.0261 ValA=0.9803\n",
            "  Extra_FFN: TrL=0.0133 TrA=0.9782  ValL=0.0072 ValA=0.9915\n",
            "  High_Dropout: TrL=0.0104 TrA=0.9843  ValL=0.0096 ValA=0.9887\n",
            "------------------------------------------------------------\n",
            "Epoch 29/50\n",
            "  Baseline_ReLU6: TrL=0.0117 TrA=0.9819  ValL=0.0053 ValA=0.9944\n",
            "  LeakyReLU: TrL=0.0120 TrA=0.9831  ValL=0.0068 ValA=0.9944\n",
            "  GeLU: TrL=0.0240 TrA=0.9674  ValL=0.0156 ValA=0.9887\n",
            "  SiLU: TrL=0.0422 TrA=0.9414  ValL=0.0154 ValA=0.9831\n",
            "  Extra_FFN: TrL=0.0116 TrA=0.9849  ValL=0.0117 ValA=0.9887\n",
            "  High_Dropout: TrL=0.0112 TrA=0.9849  ValL=0.0124 ValA=0.9915\n",
            "------------------------------------------------------------\n",
            "Epoch 30/50\n",
            "  Baseline_ReLU6: TrL=0.0140 TrA=0.9831  ValL=0.0058 ValA=0.9915\n",
            "  LeakyReLU: TrL=0.0119 TrA=0.9825  ValL=0.0059 ValA=0.9915\n",
            "  GeLU: TrL=0.0278 TrA=0.9583  ValL=0.0185 ValA=0.9887\n",
            "  SiLU: TrL=0.0349 TrA=0.9492  ValL=0.0226 ValA=0.9831\n",
            "  Extra_FFN: TrL=0.0108 TrA=0.9861  ValL=0.0109 ValA=0.9887\n",
            "  High_Dropout: TrL=0.0153 TrA=0.9789  ValL=0.0100 ValA=0.9887\n",
            "------------------------------------------------------------\n",
            "Epoch 31/50\n",
            "  Baseline_ReLU6: TrL=0.0110 TrA=0.9837  ValL=0.0038 ValA=0.9944\n",
            "  LeakyReLU: TrL=0.0124 TrA=0.9867  ValL=0.0058 ValA=0.9915\n",
            "  GeLU: TrL=0.0259 TrA=0.9619  ValL=0.0219 ValA=0.9915\n",
            "  SiLU: TrL=0.0315 TrA=0.9511  ValL=0.0174 ValA=0.9859\n",
            "  Extra_FFN: TrL=0.0091 TrA=0.9855  ValL=0.0083 ValA=0.9887\n",
            "  High_Dropout: TrL=0.0165 TrA=0.9801  ValL=0.0073 ValA=0.9915\n",
            "------------------------------------------------------------\n",
            "Epoch 32/50\n",
            "  Baseline_ReLU6: TrL=0.0088 TrA=0.9885  ValL=0.0060 ValA=0.9944\n",
            "  LeakyReLU: TrL=0.0104 TrA=0.9855  ValL=0.0063 ValA=0.9915\n",
            "  GeLU: TrL=0.0278 TrA=0.9613  ValL=0.0150 ValA=0.9887\n",
            "  SiLU: TrL=0.0301 TrA=0.9601  ValL=0.0189 ValA=0.9887\n",
            "  Extra_FFN: TrL=0.0107 TrA=0.9861  ValL=0.0095 ValA=0.9887\n",
            "  High_Dropout: TrL=0.0145 TrA=0.9825  ValL=0.0107 ValA=0.9915\n",
            "------------------------------------------------------------\n",
            "Epoch 33/50\n",
            "  Baseline_ReLU6: TrL=0.0131 TrA=0.9843  ValL=0.0041 ValA=0.9944\n",
            "  LeakyReLU: TrL=0.0128 TrA=0.9807  ValL=0.0084 ValA=0.9915\n",
            "  GeLU: TrL=0.0318 TrA=0.9553  ValL=0.0183 ValA=0.9831\n",
            "  SiLU: TrL=0.0351 TrA=0.9535  ValL=0.0199 ValA=0.9803\n",
            "  Extra_FFN: TrL=0.0126 TrA=0.9855  ValL=0.0053 ValA=0.9915\n",
            "  High_Dropout: TrL=0.0152 TrA=0.9807  ValL=0.0090 ValA=0.9944\n",
            "------------------------------------------------------------\n",
            "Epoch 34/50\n",
            "  Baseline_ReLU6: TrL=0.0117 TrA=0.9867  ValL=0.0038 ValA=0.9944\n",
            "  LeakyReLU: TrL=0.0147 TrA=0.9801  ValL=0.0106 ValA=0.9915\n",
            "  GeLU: TrL=0.0231 TrA=0.9692  ValL=0.0175 ValA=0.9887\n",
            "  SiLU: TrL=0.0305 TrA=0.9535  ValL=0.0212 ValA=0.9859\n",
            "  Extra_FFN: TrL=0.0100 TrA=0.9843  ValL=0.0051 ValA=0.9915\n",
            "  High_Dropout: TrL=0.0146 TrA=0.9807  ValL=0.0098 ValA=0.9887\n",
            "------------------------------------------------------------\n",
            "Epoch 35/50\n",
            "  Baseline_ReLU6: TrL=0.0091 TrA=0.9891  ValL=0.0042 ValA=0.9944\n",
            "  LeakyReLU: TrL=0.0133 TrA=0.9831  ValL=0.0120 ValA=0.9915\n",
            "  GeLU: TrL=0.0244 TrA=0.9698  ValL=0.0146 ValA=0.9859\n",
            "  SiLU: TrL=0.0295 TrA=0.9607  ValL=0.0262 ValA=0.9803\n",
            "  Extra_FFN: TrL=0.0106 TrA=0.9843  ValL=0.0061 ValA=0.9944\n",
            "  High_Dropout: TrL=0.0118 TrA=0.9825  ValL=0.0095 ValA=0.9915\n",
            "------------------------------------------------------------\n",
            "Epoch 36/50\n",
            "  Baseline_ReLU6: TrL=0.0137 TrA=0.9849  ValL=0.0033 ValA=0.9944\n",
            "  LeakyReLU: TrL=0.0103 TrA=0.9861  ValL=0.0059 ValA=0.9887\n",
            "  GeLU: TrL=0.0200 TrA=0.9686  ValL=0.0165 ValA=0.9887\n",
            "  SiLU: TrL=0.0222 TrA=0.9686  ValL=0.0238 ValA=0.9859\n",
            "  Extra_FFN: TrL=0.0104 TrA=0.9825  ValL=0.0039 ValA=0.9944\n",
            "  High_Dropout: TrL=0.0147 TrA=0.9825  ValL=0.0080 ValA=0.9915\n",
            "------------------------------------------------------------\n",
            "Epoch 37/50\n",
            "  Baseline_ReLU6: TrL=0.0103 TrA=0.9861  ValL=0.0050 ValA=0.9915\n",
            "  LeakyReLU: TrL=0.0091 TrA=0.9843  ValL=0.0068 ValA=0.9915\n",
            "  GeLU: TrL=0.0296 TrA=0.9583  ValL=0.0152 ValA=0.9887\n",
            "  SiLU: TrL=0.0306 TrA=0.9589  ValL=0.0216 ValA=0.9803\n",
            "  Extra_FFN: TrL=0.0104 TrA=0.9855  ValL=0.0077 ValA=0.9915\n",
            "  High_Dropout: TrL=0.0144 TrA=0.9843  ValL=0.0100 ValA=0.9915\n",
            "------------------------------------------------------------\n",
            "Epoch 38/50\n",
            "  Baseline_ReLU6: TrL=0.0135 TrA=0.9831  ValL=0.0045 ValA=0.9915\n",
            "  LeakyReLU: TrL=0.0121 TrA=0.9843  ValL=0.0054 ValA=0.9915\n",
            "  GeLU: TrL=0.0183 TrA=0.9758  ValL=0.0155 ValA=0.9887\n",
            "  SiLU: TrL=0.0319 TrA=0.9559  ValL=0.0219 ValA=0.9831\n",
            "  Extra_FFN: TrL=0.0101 TrA=0.9855  ValL=0.0061 ValA=0.9915\n",
            "  High_Dropout: TrL=0.0079 TrA=0.9891  ValL=0.0088 ValA=0.9915\n",
            "------------------------------------------------------------\n",
            "Epoch 39/50\n",
            "  Baseline_ReLU6: TrL=0.0118 TrA=0.9837  ValL=0.0032 ValA=0.9915\n",
            "  LeakyReLU: TrL=0.0093 TrA=0.9873  ValL=0.0040 ValA=0.9915\n",
            "  GeLU: TrL=0.0207 TrA=0.9692  ValL=0.0147 ValA=0.9887\n",
            "  SiLU: TrL=0.0291 TrA=0.9559  ValL=0.0263 ValA=0.9775\n",
            "  Extra_FFN: TrL=0.0116 TrA=0.9849  ValL=0.0056 ValA=0.9915\n",
            "  High_Dropout: TrL=0.0097 TrA=0.9903  ValL=0.0073 ValA=0.9915\n",
            "------------------------------------------------------------\n",
            "Epoch 40/50\n",
            "  Baseline_ReLU6: TrL=0.0143 TrA=0.9825  ValL=0.0044 ValA=0.9944\n",
            "  LeakyReLU: TrL=0.0084 TrA=0.9921  ValL=0.0050 ValA=0.9915\n",
            "  GeLU: TrL=0.0226 TrA=0.9686  ValL=0.0156 ValA=0.9887\n",
            "  SiLU: TrL=0.0333 TrA=0.9517  ValL=0.0229 ValA=0.9859\n",
            "  Extra_FFN: TrL=0.0133 TrA=0.9873  ValL=0.0050 ValA=0.9915\n",
            "  High_Dropout: TrL=0.0105 TrA=0.9855  ValL=0.0071 ValA=0.9915\n",
            "------------------------------------------------------------\n",
            "Epoch 41/50\n",
            "  Baseline_ReLU6: TrL=0.0103 TrA=0.9861  ValL=0.0038 ValA=0.9944\n",
            "  LeakyReLU: TrL=0.0105 TrA=0.9855  ValL=0.0058 ValA=0.9915\n",
            "  GeLU: TrL=0.0236 TrA=0.9692  ValL=0.0172 ValA=0.9887\n",
            "  SiLU: TrL=0.0319 TrA=0.9529  ValL=0.0214 ValA=0.9831\n",
            "  Extra_FFN: TrL=0.0077 TrA=0.9915  ValL=0.0059 ValA=0.9915\n",
            "  High_Dropout: TrL=0.0115 TrA=0.9843  ValL=0.0070 ValA=0.9915\n",
            "------------------------------------------------------------\n",
            "Epoch 42/50\n",
            "  Baseline_ReLU6: TrL=0.0143 TrA=0.9831  ValL=0.0030 ValA=0.9915\n",
            "  LeakyReLU: TrL=0.0103 TrA=0.9861  ValL=0.0056 ValA=0.9915\n",
            "  GeLU: TrL=0.0187 TrA=0.9746  ValL=0.0182 ValA=0.9859\n",
            "  SiLU: TrL=0.0294 TrA=0.9589  ValL=0.0223 ValA=0.9803\n",
            "  Extra_FFN: TrL=0.0107 TrA=0.9891  ValL=0.0063 ValA=0.9915\n",
            "  High_Dropout: TrL=0.0100 TrA=0.9897  ValL=0.0065 ValA=0.9915\n",
            "------------------------------------------------------------\n",
            "Epoch 43/50\n",
            "  Baseline_ReLU6: TrL=0.0088 TrA=0.9873  ValL=0.0035 ValA=0.9944\n",
            "  LeakyReLU: TrL=0.0097 TrA=0.9843  ValL=0.0069 ValA=0.9915\n",
            "  GeLU: TrL=0.0213 TrA=0.9728  ValL=0.0175 ValA=0.9887\n",
            "  SiLU: TrL=0.0276 TrA=0.9631  ValL=0.0240 ValA=0.9859\n",
            "  Extra_FFN: TrL=0.0066 TrA=0.9934  ValL=0.0061 ValA=0.9915\n",
            "  High_Dropout: TrL=0.0116 TrA=0.9843  ValL=0.0079 ValA=0.9887\n",
            "------------------------------------------------------------\n",
            "Epoch 44/50\n",
            "  Baseline_ReLU6: TrL=0.0100 TrA=0.9855  ValL=0.0036 ValA=0.9944\n",
            "  LeakyReLU: TrL=0.0069 TrA=0.9879  ValL=0.0059 ValA=0.9915\n",
            "  GeLU: TrL=0.0274 TrA=0.9644  ValL=0.0176 ValA=0.9859\n",
            "  SiLU: TrL=0.0249 TrA=0.9656  ValL=0.0221 ValA=0.9859\n",
            "  Extra_FFN: TrL=0.0086 TrA=0.9873  ValL=0.0066 ValA=0.9915\n",
            "  High_Dropout: TrL=0.0095 TrA=0.9861  ValL=0.0090 ValA=0.9887\n",
            "------------------------------------------------------------\n",
            "Epoch 45/50\n",
            "  Baseline_ReLU6: TrL=0.0092 TrA=0.9861  ValL=0.0036 ValA=0.9944\n",
            "  LeakyReLU: TrL=0.0108 TrA=0.9843  ValL=0.0071 ValA=0.9915\n",
            "  GeLU: TrL=0.0274 TrA=0.9662  ValL=0.0164 ValA=0.9887\n",
            "  SiLU: TrL=0.0299 TrA=0.9559  ValL=0.0221 ValA=0.9831\n",
            "  Extra_FFN: TrL=0.0086 TrA=0.9903  ValL=0.0070 ValA=0.9915\n",
            "  High_Dropout: TrL=0.0076 TrA=0.9897  ValL=0.0086 ValA=0.9887\n",
            "------------------------------------------------------------\n",
            "Epoch 46/50\n",
            "  Baseline_ReLU6: TrL=0.0098 TrA=0.9879  ValL=0.0035 ValA=0.9944\n",
            "  LeakyReLU: TrL=0.0082 TrA=0.9897  ValL=0.0073 ValA=0.9915\n",
            "  GeLU: TrL=0.0216 TrA=0.9704  ValL=0.0170 ValA=0.9859\n",
            "  SiLU: TrL=0.0312 TrA=0.9553  ValL=0.0220 ValA=0.9859\n",
            "  Extra_FFN: TrL=0.0096 TrA=0.9897  ValL=0.0069 ValA=0.9915\n",
            "  High_Dropout: TrL=0.0107 TrA=0.9819  ValL=0.0102 ValA=0.9915\n",
            "------------------------------------------------------------\n",
            "Epoch 47/50\n",
            "  Baseline_ReLU6: TrL=0.0076 TrA=0.9891  ValL=0.0032 ValA=0.9915\n",
            "  LeakyReLU: TrL=0.0106 TrA=0.9873  ValL=0.0080 ValA=0.9915\n",
            "  GeLU: TrL=0.0265 TrA=0.9668  ValL=0.0163 ValA=0.9887\n",
            "  SiLU: TrL=0.0277 TrA=0.9619  ValL=0.0225 ValA=0.9803\n",
            "  Extra_FFN: TrL=0.0052 TrA=0.9927  ValL=0.0066 ValA=0.9915\n",
            "  High_Dropout: TrL=0.0126 TrA=0.9831  ValL=0.0083 ValA=0.9915\n",
            "------------------------------------------------------------\n",
            "Epoch 48/50\n",
            "  Baseline_ReLU6: TrL=0.0127 TrA=0.9813  ValL=0.0039 ValA=0.9944\n",
            "  LeakyReLU: TrL=0.0104 TrA=0.9843  ValL=0.0058 ValA=0.9915\n",
            "  GeLU: TrL=0.0195 TrA=0.9740  ValL=0.0159 ValA=0.9887\n",
            "  SiLU: TrL=0.0286 TrA=0.9668  ValL=0.0232 ValA=0.9859\n",
            "  Extra_FFN: TrL=0.0071 TrA=0.9891  ValL=0.0069 ValA=0.9915\n",
            "  High_Dropout: TrL=0.0106 TrA=0.9891  ValL=0.0089 ValA=0.9915\n",
            "------------------------------------------------------------\n",
            "Epoch 49/50\n",
            "  Baseline_ReLU6: TrL=0.0078 TrA=0.9915  ValL=0.0030 ValA=0.9944\n",
            "  LeakyReLU: TrL=0.0076 TrA=0.9885  ValL=0.0050 ValA=0.9915\n",
            "  GeLU: TrL=0.0175 TrA=0.9752  ValL=0.0157 ValA=0.9859\n",
            "  SiLU: TrL=0.0265 TrA=0.9680  ValL=0.0216 ValA=0.9859\n",
            "  Extra_FFN: TrL=0.0094 TrA=0.9879  ValL=0.0069 ValA=0.9915\n",
            "  High_Dropout: TrL=0.0102 TrA=0.9867  ValL=0.0076 ValA=0.9887\n",
            "------------------------------------------------------------\n",
            "Epoch 50/50\n",
            "  Baseline_ReLU6: TrL=0.0089 TrA=0.9873  ValL=0.0040 ValA=0.9944\n",
            "  LeakyReLU: TrL=0.0104 TrA=0.9861  ValL=0.0067 ValA=0.9915\n",
            "  GeLU: TrL=0.0198 TrA=0.9716  ValL=0.0159 ValA=0.9887\n",
            "  SiLU: TrL=0.0261 TrA=0.9631  ValL=0.0212 ValA=0.9859\n",
            "  Extra_FFN: TrL=0.0106 TrA=0.9885  ValL=0.0063 ValA=0.9915\n",
            "  High_Dropout: TrL=0.0121 TrA=0.9843  ValL=0.0072 ValA=0.9887\n",
            "------------------------------------------------------------\n",
            "\n",
            "Final Test Results:\n",
            "Baseline_ReLU6: TestL=0.0020  TestA=0.9972\n",
            "LeakyReLU: TestL=0.0017  TestA=0.9972\n",
            "GeLU: TestL=0.0026  TestA=1.0000\n",
            "SiLU: TestL=0.0053  TestA=0.9944\n",
            "Extra_FFN: TestL=0.0017  TestA=0.9972\n",
            "High_Dropout: TestL=0.0019  TestA=0.9972\n"
          ]
        }
      ],
      "source": [
        "#%%\n",
        "# 2) Use the exact Drive paths for your reef & sand folders\n",
        "reef_dir = \"/content/drive/MyDrive/Tamu(grad)/CSCE753 CVRP/project test model implementation/reef\"\n",
        "sand_dir = \"/content/drive/MyDrive/Tamu(grad)/CSCE753 CVRP/project test model implementation/sand\"\n",
        "\n",
        "# Debug\n",
        "print(\"reef_dir contents:\", os.listdir(reef_dir))\n",
        "print(\"sand_dir contents:\", os.listdir(sand_dir))\n",
        "\n",
        "# get images\n",
        "reef_files = []\n",
        "for root, _, files in os.walk(reef_dir):\n",
        "    for f in files:\n",
        "        if f.lower().endswith(\".png\"):\n",
        "            reef_files.append(os.path.join(root, f))\n",
        "\n",
        "sand_files = []\n",
        "for root, _, files in os.walk(sand_dir):\n",
        "    for f in files:\n",
        "        if f.lower().endswith(\".png\"):\n",
        "            sand_files.append(os.path.join(root, f))\n",
        "\n",
        "print(f\"Found {len(reef_files)} reef images and {len(sand_files)} sand images\")\n",
        "\n",
        "all_files  = reef_files + sand_files\n",
        "all_labels = [0]*len(reef_files) + [1]*len(sand_files)\n",
        "\n",
        "#%%\n",
        "# 3) Split into train/val/test (70/15/15)\n",
        "train_files, temp_files, train_labels, temp_labels = train_test_split(\n",
        "    all_files, all_labels, test_size=0.30, stratify=all_labels, random_state=42)\n",
        "\n",
        "val_files, test_files, val_labels, test_labels = train_test_split(\n",
        "    temp_files, temp_labels, test_size=0.50, stratify=temp_labels, random_state=42)\n",
        "\n",
        "print(\"Train:\", len(train_files), \"Val:\", len(val_files), \"Test:\", len(test_files))\n",
        "\n",
        "#%%\n",
        "# 4) Transforms ver 01\n",
        "# train_transform = transforms.Compose([\n",
        "#     transforms.Resize(256),\n",
        "#     transforms.RandomResizedCrop(224),\n",
        "#     transforms.RandomHorizontalFlip(),\n",
        "#     transforms.ColorJitter(0.3,0.3,0.3,0.15),\n",
        "#     transforms.RandomErasing(p=0.5, scale=(0.02,0.2), ratio=(0.3,3.3), value='random'),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "# ])\n",
        "# val_transform = transforms.Compose([\n",
        "#     transforms.Resize(256),\n",
        "#     transforms.CenterCrop(224),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "# ])\n",
        "\n",
        "# 4) Transforms ver 02\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(0.3, 0.3, 0.3, 0.15),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomErasing(\n",
        "        p=0.5,\n",
        "        scale=(0.02, 0.2),\n",
        "        ratio=(0.3, 3.3),\n",
        "        value='random'\n",
        "    ),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "# 5) Dataset & DataLoader\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, files, labels, transform=None):\n",
        "        self.files, self.labels, self.transform = files, labels, transform\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.files[idx]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, self.labels[idx]\n",
        "\n",
        "train_ds = CustomImageDataset(train_files, train_labels, transform=train_transform)\n",
        "val_ds   = CustomImageDataset(val_files,   val_labels,   transform=val_transform)\n",
        "test_ds  = CustomImageDataset(test_files,  test_labels,  transform=val_transform)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  num_workers=2)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False, num_workers=2)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "# 6) Models\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2, weight=None, reduction=\"mean\"):\n",
        "        super().__init__()\n",
        "        self.gamma, self.reduction = gamma, reduction\n",
        "        self.ce = nn.CrossEntropyLoss(weight=weight, reduction=\"none\")\n",
        "    def forward(self, x, y):\n",
        "        logpt = -self.ce(x, y)\n",
        "        pt = torch.exp(logpt)\n",
        "        loss = -((1 - pt)**self.gamma) * logpt\n",
        "        return loss.mean() if self.reduction==\"mean\" else loss.sum() if self.reduction==\"sum\" else loss\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, c, ratio=16):\n",
        "        super().__init__()\n",
        "        self.avg = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max = nn.AdaptiveMaxPool2d(1)\n",
        "        self.fc  = nn.Sequential(\n",
        "            nn.Conv2d(c, c//ratio, 1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(c//ratio, c, 1, bias=False)\n",
        "        )\n",
        "        self.sig = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        return x * self.sig(self.fc(self.avg(x)) + self.fc(self.max(x)))\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, k=7):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(2,1,k,padding=(k-1)//2,bias=False)\n",
        "        self.sig  = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        avg = x.mean(1,keepdim=True)\n",
        "        mx,_ = x.max(1,keepdim=True)\n",
        "        return x * self.sig(self.conv(torch.cat([avg, mx],1)))\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, c):\n",
        "        super().__init__()\n",
        "        self.ca = ChannelAttention(c)\n",
        "        self.sa = SpatialAttention()\n",
        "    def forward(self, x):\n",
        "        return self.sa(self.ca(x))\n",
        "\n",
        "def replace_relu6(module, new_act):\n",
        "    for name, child in module.named_children():\n",
        "        if isinstance(child, nn.ReLU6):\n",
        "            setattr(module, name, new_act())\n",
        "        else:\n",
        "            replace_relu6(child, new_act)\n",
        "\n",
        "def get_act(name):\n",
        "    return {\n",
        "        \"relu6\": nn.ReLU6,\n",
        "        \"leaky\": lambda: nn.LeakyReLU(0.01,inplace=True),\n",
        "        \"gelu\": nn.GELU,\n",
        "        \"silu\": nn.SiLU\n",
        "    }[name.lower()]\n",
        "\n",
        "class CustomMobileNetV2(nn.Module):\n",
        "    def __init__(self, num_classes=2, activation=\"relu6\",\n",
        "                 use_cbam=False, extra_fc=False,\n",
        "                 dropout_rate=0.2, fc_dim=128):\n",
        "        super().__init__()\n",
        "        self.net = models.mobilenet_v2(pretrained=True)\n",
        "        act = get_act(activation)\n",
        "        replace_relu6(self.net, act)\n",
        "        c = self.net.last_channel\n",
        "        if use_cbam: self.cbam = CBAM(c)\n",
        "        if extra_fc:\n",
        "            self.net.classifier = nn.Sequential(\n",
        "                nn.Dropout(dropout_rate),\n",
        "                nn.Linear(c, fc_dim),\n",
        "                act(),\n",
        "                nn.Linear(fc_dim, num_classes)\n",
        "            )\n",
        "        else:\n",
        "            self.net.classifier = nn.Sequential(\n",
        "                nn.Dropout(dropout_rate),\n",
        "                nn.Linear(c, num_classes)\n",
        "            )\n",
        "    def forward(self, x):\n",
        "        x = self.net.features(x).mean([2,3])\n",
        "        if hasattr(self, \"cbam\"):\n",
        "            xb = self.cbam(x.unsqueeze(-1).unsqueeze(-1))\n",
        "            x = xb.view(x.size(0), -1)\n",
        "        return self.net.classifier(x)\n",
        "\n",
        "\n",
        "# 7) optimizers, schedulers, scalers\n",
        "num_epochs = 50\n",
        "patience   = 5\n",
        "\n",
        "models_dict = {\n",
        "    \"Baseline_ReLU6\": CustomMobileNetV2(activation=\"relu6\"),\n",
        "    \"LeakyReLU\":       CustomMobileNetV2(activation=\"leaky\"),\n",
        "    \"GeLU\":            CustomMobileNetV2(activation=\"gelu\"),\n",
        "    \"SiLU\":            CustomMobileNetV2(activation=\"silu\"),\n",
        "    \"Extra_FFN\":       CustomMobileNetV2(activation=\"relu6\", use_cbam=True, extra_fc=True, fc_dim=256),\n",
        "    \"High_Dropout\":    CustomMobileNetV2(activation=\"relu6\", dropout_rate=0.7),\n",
        "}\n",
        "\n",
        "optimizers = {}\n",
        "schedulers = {}\n",
        "scalers    = {}\n",
        "criterion  = FocalLoss(gamma=2)\n",
        "\n",
        "for name, m in models_dict.items():\n",
        "    m.to(device)\n",
        "    optimizers[name] = optim.AdamW(m.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "    schedulers[name] = optim.lr_scheduler.CosineAnnealingLR(optimizers[name], T_max=num_epochs)\n",
        "    scalers[name]    = GradScaler()\n",
        "\n",
        "best_val = {n:1e9 for n in models_dict}\n",
        "no_imp   = {n:0   for n in models_dict}\n",
        "\n",
        "history = {n:{'train_loss':[], 'train_acc':[], 'val_loss':[], 'val_acc':[]} for n in models_dict}\n",
        "\n",
        "# 8) Train\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    print(f\"Epoch {epoch}/{num_epochs}\")\n",
        "    for name, model in models_dict.items():\n",
        "        model.train()\n",
        "        running_loss, total, correct = 0.0, 0, 0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizers[name].zero_grad()\n",
        "            with autocast():\n",
        "                out  = model(xb)\n",
        "                loss = criterion(out, yb)\n",
        "            scalers[name].scale(loss).backward()\n",
        "            scalers[name].step(optimizers[name])\n",
        "            scalers[name].update()\n",
        "\n",
        "            running_loss += loss.item() * xb.size(0)\n",
        "            total       += yb.size(0)\n",
        "            correct     += (out.argmax(1) == yb).sum().item()\n",
        "\n",
        "        train_loss = running_loss / total\n",
        "        train_acc  = correct / total\n",
        "\n",
        "        # -=-=-=-=-=-=-= Validate -=-=-=-=-=-=-=-=-=-=-=\n",
        "        model.eval()\n",
        "        val_loss, vtotal, vcorrect = 0.0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                out  = model(xb)\n",
        "                loss = criterion(out, yb)\n",
        "\n",
        "                val_loss += loss.item() * xb.size(0)\n",
        "                vtotal   += yb.size(0)\n",
        "                vcorrect += (out.argmax(1) == yb).sum().item()\n",
        "\n",
        "        val_loss /= vtotal\n",
        "        val_acc   = vcorrect / vtotal\n",
        "\n",
        "        # -=-=-=-=-=-=-=-=-=-= scheduler -=-=-=-=-=-=-=-=-=-=-=-\n",
        "        history[name]['train_loss'].append(train_loss)\n",
        "        history[name]['train_acc'].append(train_acc)\n",
        "        history[name]['val_loss'].append(val_loss)\n",
        "        history[name]['val_acc'].append(val_acc)\n",
        "\n",
        "        print(f\"  {name}: TrL={train_loss:.4f} TrA={train_acc:.4f}  ValL={val_loss:.4f} ValA={val_acc:.4f}\")\n",
        "\n",
        "        schedulers[name].step()\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# 9) infernece\n",
        "print(\"\\nFinal Test Results:\")\n",
        "for name, model in models_dict.items():\n",
        "    model.eval()\n",
        "    test_loss, ttotal, tcorrect = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            out  = model(xb)\n",
        "            loss = criterion(out, yb)\n",
        "            test_loss += loss.item() * xb.size(0)\n",
        "            ttotal    += yb.size(0)\n",
        "            tcorrect  += (out.argmax(1) == yb).sum().item()\n",
        "    print(f\"{name}: TestL={test_loss/ttotal:.4f}  TestA={tcorrect/ttotal:.4f}\")"
      ]
    }
  ]
}